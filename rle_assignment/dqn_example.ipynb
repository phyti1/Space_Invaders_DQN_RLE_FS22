{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11127b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc12374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if(os.path.exists(\"./persistent\")):\n",
    "    os.chdir(\"./persistent\")\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from absl import flags, app\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "\n",
    "import rle_assignment.env\n",
    "from rle_assignment.utils import LinearSchedule, RingBuffer\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for key in keys_list:\n",
    "        # ignore default flags\n",
    "        if(key not in ['logtostderr', 'alsologtostderr', 'log_dir', 'v', 'verbosity', 'logger_levels', 'stderrthreshold', 'showprefixforinfo', 'run_with_pdb', 'pdb_post_mortem', 'pdb', 'run_with_profiling', 'profile_file', 'use_cprofile_for_profiling', 'only_check_args']):\n",
    "            FLAGS.__delattr__(key)\n",
    "\n",
    "# try to clear all flags to be able to rerun\n",
    "try:\n",
    "    del_all_flags(flags.FLAGS)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# common flags\n",
    "flags.DEFINE_enum('mode', 'train', ['train', 'eval'], 'Run mode.')\n",
    "flags.DEFINE_string('logdir', './runs', 'Directory where all outputs are written to.')\n",
    "flags.DEFINE_string('run_name', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), 'Run name.')\n",
    "flags.DEFINE_bool('cuda', False, 'Whether to run the model on gpu or on cpu.')\n",
    "flags.DEFINE_integer('seed', 42, 'Random seed.')\n",
    "\n",
    "# train flags\n",
    "flags.DEFINE_float('gamma', .99, 'Discount factor.')\n",
    "flags.DEFINE_integer('batch_size', 32, 'Train batch size.')\n",
    "flags.DEFINE_float('learning_rate', 2.5e-4, 'Learning rate.')\n",
    "flags.DEFINE_float('max_grad_norm', 10, 'Maximum gradient norm. Gradients with larger norms will be clipped.')\n",
    "flags.DEFINE_integer('num_envs', 2, 'Number of parallel env processes.')\n",
    "flags.DEFINE_integer('total_steps', 10_000_000, 'Total number of agent steps.')\n",
    "flags.DEFINE_integer('warmup_steps', 80_000, 'Number of warmup steps to fill the replay buffer.')\n",
    "flags.DEFINE_integer('buffer_size', 100_000, 'Replay buffer size.')\n",
    "flags.DEFINE_float('exploration_epsilon_initial', 1.0, 'Initial exploration rate.')\n",
    "flags.DEFINE_float('exploration_epsilon_final', 0.1, 'Final exploration rate.')\n",
    "flags.DEFINE_float('exploration_fraction', 0.1, 'Fraction of total_frames it takes to decay initial to final epsilon.')\n",
    "flags.DEFINE_integer('train_freq', 4, 'Frequency at which train steps are executed.')\n",
    "flags.DEFINE_integer('checkpoint_freq', 100_000, 'Frequency at which checkpoints are stored.')\n",
    "flags.DEFINE_integer('logging_freq', 10_000, 'Frequency at which logs are written.')\n",
    "flags.DEFINE_integer('target_network_update_freq', 1000, 'Frequency at which the target network is updated.')\n",
    "\n",
    "# eval flags\n",
    "flags.DEFINE_string('eval_checkpoint', 'checkpoint-3500000.pt', 'Eval checkpoint filename.')\n",
    "flags.DEFINE_integer('eval_num_episodes', 30, 'Number of eval episodes.')\n",
    "flags.DEFINE_float('eval_epsilon', 0.05, 'Epsilon-greedy during eval.')\n",
    "flags.DEFINE_bool('eval_render', False, 'Render env during eval.')\n",
    "flags.DEFINE_integer('eval_seed', 1234, 'Eval seed.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def make_env_fn(seed: int, render_human: bool = False, video_folder: Optional[str] = None) -> Callable[[], gym.Env]:\n",
    "    \"\"\" returns a pickleable callable to create an env instance \"\"\"\n",
    "    def env_fn():\n",
    "        env = rle_assignment.env.make_env(render_human, video_folder)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.TransformObservation(env, np.squeeze)  # get rid of 3rd dimension added by ResizeObservation\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return env_fn\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions: int, in_channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.qnet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = torch.unsqueeze(obs, dim=1)  # add channel dim\n",
    "        obs = obs * (1. / 255.)\n",
    "        return self.qnet(obs)\n",
    "\n",
    "\n",
    "def train(device):\n",
    "    random.seed(FLAGS.seed)\n",
    "    np.random.seed(FLAGS.seed)\n",
    "    torch.manual_seed(FLAGS.seed)\n",
    "\n",
    "    logdir = os.path.join(FLAGS.logdir, FLAGS.run_name)\n",
    "    try:\n",
    "        os.makedirs(logdir, exist_ok=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    FLAGS.append_flags_into_file(os.path.join(logdir, 'flags.txt'))\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(logdir, 'logs'))\n",
    "    writer.add_text(\"config\", FLAGS.flags_into_string())\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([\n",
    "        make_env_fn(seed=FLAGS.seed, video_folder=os.path.join(logdir, 'videos', 'train') if i == 0 else None)\n",
    "        for i in range(FLAGS.num_envs)])\n",
    "\n",
    "    env_name = envs.get_attr('spec')[0].name\n",
    "\n",
    "    exploration_epsilon_schedule = LinearSchedule(\n",
    "        initial_value=1.,\n",
    "        final_value=FLAGS.exploration_epsilon_final,\n",
    "        schedule_steps=int(FLAGS.exploration_fraction * FLAGS.total_steps)\n",
    "    )\n",
    "\n",
    "    q_network = DQN(envs.single_action_space.n).to(device)\n",
    "\n",
    "    target_network = DQN(envs.single_action_space.n).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=FLAGS.learning_rate)\n",
    "\n",
    "    replay_buffer = RingBuffer(size=FLAGS.buffer_size, specs={\n",
    "        'obs': (envs.single_observation_space.shape, envs.single_observation_space.dtype),\n",
    "        'next_obs': (envs.single_observation_space.shape, envs.single_observation_space.dtype),\n",
    "        'actions': (envs.single_action_space.shape, envs.single_action_space.dtype),\n",
    "        'rewards': ((), np.float32),\n",
    "        'dones': ((), np.float32),\n",
    "    })\n",
    "\n",
    "    logs = defaultdict(list)\n",
    "    last_log_frame = 0\n",
    "    last_log_time = time.time()\n",
    "    total_frames = 0\n",
    "\n",
    "    obs = envs.reset()\n",
    "\n",
    "    for global_step in range(FLAGS.total_steps):\n",
    "        # epsilon greedy action selection\n",
    "        epsilon = exploration_epsilon_schedule.value(global_step)\n",
    "        if random.random() < epsilon:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            q_values = q_network(torch.tensor(obs).to(device))\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "        logs[\"epsilon\"].append(epsilon)\n",
    "\n",
    "        # execute actions in environment\n",
    "        new_obs, rewards, dones, infos = envs.step(actions)\n",
    "        for done, info in zip(dones, infos):\n",
    "            print(f\"info: {info}\")\n",
    "            if done and \"episode\" in info.keys():\n",
    "                logs[f\"{env_name}/episode_frames\"].append(info[\"episode_frame_number\"])\n",
    "                logs[f\"{env_name}/episode_reward\"].append(info[\"episode\"][\"r\"])\n",
    "                logs[f\"{env_name}/episode_steps\"].append(info[\"episode\"][\"l\"])\n",
    "                total_frames += info[\"episode_frame_number\"]\n",
    "\n",
    "        # vector envs reset automatically, so we have to manually get the terminal observations for these steps\n",
    "        next_obs = new_obs.copy()\n",
    "        for i, done in enumerate(dones):\n",
    "            if done and infos[i].get(\"terminal_observation\") is not None:\n",
    "                next_obs[i] = infos[i][\"terminal_observation\"]\n",
    "\n",
    "        # save data to reply buffer\n",
    "        replay_buffer.put({\n",
    "            'obs': obs,\n",
    "            'next_obs': next_obs,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'dones': dones,\n",
    "        })\n",
    "\n",
    "        # optimize model (after initial warmup phase to fill the replay buffer)\n",
    "        if global_step > FLAGS.warmup_steps and global_step % FLAGS.train_freq == 0:\n",
    "            # sample a batch from the replay buffer\n",
    "            batch = {\n",
    "                k: torch.tensor(v).to(device)\n",
    "                for k, v in replay_buffer.sample(FLAGS.batch_size).items()\n",
    "            }\n",
    "\n",
    "            # compute estimate of best q values starting from next states\n",
    "            next_q_value, _ = target_network(batch['next_obs']).max(dim=1)\n",
    "\n",
    "            # mask q values where the episode has ended at the current step\n",
    "            next_q_value_masked = next_q_value * (1 - batch['dones'])\n",
    "\n",
    "            # compute td target\n",
    "            td_target = batch['rewards'] + FLAGS.gamma * next_q_value_masked\n",
    "\n",
    "            # compute estimated q values of actions taken in current step\n",
    "            selected_q_values = q_network(batch['obs']).gather(1, torch.unsqueeze(batch['actions'], dim=1))\n",
    "\n",
    "            # compute loss (huber loss)\n",
    "            loss = F.smooth_l1_loss(selected_q_values.squeeze(), td_target.detach())\n",
    "\n",
    "            # optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(list(q_network.parameters()), FLAGS.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            logs['loss'].append(loss.cpu().detach().numpy())\n",
    "            logs['q_values'].append(selected_q_values.mean().item())\n",
    "            logs['grad_norm'].append(grad_norm.cpu().detach().numpy())\n",
    "\n",
    "        # set obs to new obs for next step\n",
    "        obs = new_obs\n",
    "\n",
    "        # update the target network\n",
    "        if global_step > FLAGS.warmup_steps and global_step % FLAGS.target_network_update_freq == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # store checkpoint\n",
    "        if global_step > FLAGS.warmup_steps and global_step % FLAGS.checkpoint_freq == 0:\n",
    "            torch.save(q_network.state_dict(), os.path.join(logdir, f'checkpoint-{global_step}.pt'))\n",
    "\n",
    "        # logging\n",
    "        if global_step % FLAGS.logging_freq == 0:\n",
    "            current_log_time = time.time()\n",
    "            fps = (total_frames - last_log_frame) / (current_log_time - last_log_time)\n",
    "            writer.add_scalar(\"fps\", fps, total_frames)\n",
    "            writer.add_scalar('steps', global_step, total_frames)\n",
    "            for k, v in logs.items():\n",
    "                writer.add_scalar(f'{k}/mean', np.mean(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/std', np.std(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/min', np.min(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/max', np.max(v), total_frames)\n",
    "            logs['fps'].append(fps)\n",
    "            print(\" | \".join([f\"step={global_step}\"] + [f\"{k}={np.mean(v):.2f}\" for k, v in sorted(logs.items())]))\n",
    "            logs = defaultdict(list)\n",
    "            last_log_frame = total_frames\n",
    "            last_log_time = current_log_time\n",
    "\n",
    "    envs.close()\n",
    "    torch.save(q_network.state_dict(), os.path.join(logdir, f'checkpoint-last.pt'))\n",
    "\n",
    "\n",
    "def eval(device):\n",
    "    env_fn = make_env_fn(FLAGS.eval_seed, FLAGS.eval_render)\n",
    "    env = env_fn()\n",
    "\n",
    "    q_network = DQN(env.action_space.n).to(device)\n",
    "    q_network.load_state_dict(torch.load(os.path.join(FLAGS.logdir, FLAGS.run_name, FLAGS.eval_checkpoint)))\n",
    "    q_network.eval()\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode_idx in range(FLAGS.eval_num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            q_values = q_network(torch.tensor([obs]).to(device))\n",
    "\n",
    "            if random.random() < FLAGS.eval_epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = int(torch.argmax(q_values, dim=1).cpu().numpy())\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode_idx}: \"\n",
    "                      f\"reward={info['episode']['r']}, \"\n",
    "                      f\"steps={step}, \"\n",
    "                      f\"frames={info['episode_frame_number']}\")\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    print(f\"Evaluation completed: \"\n",
    "          f\"mean_episode_reward={np.mean(episode_rewards):.2f}, \"\n",
    "          f\"std_episode_reward={np.std(episode_rewards):.2f}, \"\n",
    "          f\"min_episode_reward={np.min(episode_rewards):.2f}, \"\n",
    "          f\"max_episode_reward={np.max(episode_rewards):.2f}\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd99675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.argv)\n",
    "# remove jupyter cmdline args\n",
    "sys.argv = list([sys.argv[0]])\n",
    "print(sys.argv)\n",
    "# custom flags:\n",
    "print(np.array(FLAGS))\n",
    "\n",
    "def main(_):\n",
    "    device_name = \"cuda\" if FLAGS.cuda else \"cpu\"\n",
    "    if device_name == \"cuda\" and not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"cuda=true, but cuda is not available\")\n",
    "    device = torch.device(device_name)\n",
    "    print(f\"Using device: {device_name}\")\n",
    "\n",
    "    if FLAGS.mode == 'train':\n",
    "        train(device)\n",
    "    elif FLAGS.mode == 'eval':\n",
    "        eval(device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf187b75cc3fb8ab9aa91773336101b1de52e975f089fdec543117d3a8f9f64"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 ('rle_MC1-L-IjOXmM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
