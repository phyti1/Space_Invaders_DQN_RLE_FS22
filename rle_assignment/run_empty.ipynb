{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f29891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if(os.path.exists(\"./persistent\")):\n",
    "    os.chdir(\"./persistent\")\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from absl import flags, app\n",
    "from tensorboardX import SummaryWriter\n",
    "import rle_assignment.env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539573e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for key in keys_list:\n",
    "        # ignore default flags\n",
    "        if(key not in ['logtostderr', 'alsologtostderr', 'log_dir', 'v', 'verbosity', 'logger_levels', 'stderrthreshold', 'showprefixforinfo', 'run_with_pdb', 'pdb_post_mortem', 'pdb', 'run_with_profiling', 'profile_file', 'use_cprofile_for_profiling', 'only_check_args']):\n",
    "            FLAGS.__delattr__(key)\n",
    "\n",
    "# try to clear all flags to be able to rerun\n",
    "try:\n",
    "    del_all_flags(flags.FLAGS)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# common flags\n",
    "flags.DEFINE_enum('mode', 'train', ['train', 'eval'], 'Run mode.')\n",
    "flags.DEFINE_string('logdir', './runs', 'Directory where all outputs are written to.')\n",
    "flags.DEFINE_string('run_name', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), 'Run name.')\n",
    "flags.DEFINE_bool('cuda', True, 'Whether to run the model on gpu or on cpu.')\n",
    "flags.DEFINE_integer('seed', 42, 'Random seed.')\n",
    "\n",
    "# train flags\n",
    "# TODO: add your train parameters here\n",
    "flags.DEFINE_integer('num_envs', 2, 'Number of parallel env processes.')\n",
    "flags.DEFINE_integer('total_steps', 10_000_000, 'Total number of agent steps.')\n",
    "flags.DEFINE_integer('checkpoint_freq', 100_000, 'Frequency at which checkpoints are stored.')\n",
    "flags.DEFINE_integer('logging_freq', 10_000, 'Frequency at which logs are written.')\n",
    "\n",
    "# eval flags\n",
    "# TODO: add your eval parameters here\n",
    "flags.DEFINE_integer('eval_num_episodes', 30, 'Number of eval episodes.')\n",
    "flags.DEFINE_bool('eval_render', False, 'Render env during eval.')\n",
    "flags.DEFINE_integer('eval_seed', 1234, 'Eval seed.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_fn(seed: int, render_human: bool = False, video_folder: Optional[str] = None) -> Callable[[], gym.Env]:\n",
    "    \"\"\" returns a pickleable callable to create an env instance \"\"\"\n",
    "    def env_fn():\n",
    "        env = rle_assignment.env.make_env(render_human, video_folder)\n",
    "\n",
    "        #region: maybe add other gym.wrappers\n",
    "\n",
    "\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return env_fn\n",
    "\n",
    "def train():\n",
    "    random.seed(FLAGS.seed)\n",
    "    np.random.seed(FLAGS.seed)\n",
    "\n",
    "    logdir = os.path.join(FLAGS.logdir, FLAGS.run_name)\n",
    "    os.makedirs(logdir, exist_ok=False)\n",
    "\n",
    "    FLAGS.append_flags_into_file(os.path.join(logdir, 'flags.txt'))\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(logdir, 'logs'))\n",
    "    writer.add_text(\"config\", FLAGS.flags_into_string())\n",
    "\n",
    "    # initialize environments\n",
    "    envs = gym.vector.AsyncVectorEnv([\n",
    "        make_env_fn(seed=FLAGS.seed, video_folder=os.path.join(logdir, 'videos', 'train') if i == 0 else None)\n",
    "        for i in range(FLAGS.num_envs)])\n",
    "\n",
    "    env_name = envs.get_attr('spec')[0].name\n",
    "\n",
    "    #region: initialize agent, algorithm, etc...\n",
    "    \n",
    "\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    logs = defaultdict(list)\n",
    "    last_log_frame = 0\n",
    "    last_log_time = time.time()\n",
    "    total_frames = 0\n",
    "\n",
    "    obs = envs.reset()\n",
    "\n",
    "    for global_step in range(FLAGS.total_steps):\n",
    "\n",
    "        #region: select actions (one for each environment)\n",
    "\n",
    "\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        # execute actions in environment\n",
    "        new_obs, rewards, dones, infos = envs.step(actions)\n",
    "        for done, info in zip(dones, infos):\n",
    "            if done and \"episode\" in info.keys():\n",
    "                logs[f\"{env_name}/episode_frames\"].append(info[\"episode_frame_number\"])\n",
    "                logs[f\"{env_name}/episode_reward\"].append(info[\"episode\"][\"r\"])\n",
    "                logs[f\"{env_name}/episode_steps\"].append(info[\"episode\"][\"l\"])\n",
    "                total_frames += info[\"episode_frame_number\"]\n",
    "\n",
    "        # vector envs reset automatically, so we have to manually get the terminal observations for these steps\n",
    "        next_obs = new_obs.copy()\n",
    "        for i, done in enumerate(dones):\n",
    "            if done and infos[i].get(\"terminal_observation\") is not None:\n",
    "                next_obs[i] = infos[i][\"terminal_observation\"]\n",
    "\n",
    "        #region: update agent\n",
    "\n",
    "\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        # set obs to new obs for next step\n",
    "        obs = new_obs\n",
    "\n",
    "        # store checkpoint\n",
    "        if global_step % FLAGS.checkpoint_freq == 0:\n",
    "\n",
    "            #region: save agent checkpoint\n",
    "\n",
    "\n",
    "\n",
    "            #endregion\n",
    "\n",
    "            pass\n",
    "\n",
    "        # logging\n",
    "        if global_step % FLAGS.logging_freq == 0:\n",
    "            current_log_time = time.time()\n",
    "            fps = (total_frames - last_log_frame) / (current_log_time - last_log_time)\n",
    "            writer.add_scalar(\"fps\", fps, total_frames)\n",
    "            writer.add_scalar('steps', global_step, total_frames)\n",
    "            for k, v in logs.items():\n",
    "                writer.add_scalar(f'{k}/mean', np.mean(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/std', np.std(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/min', np.min(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/max', np.max(v), total_frames)\n",
    "            logs['fps'].append(fps)\n",
    "            print(\" | \".join([f\"step={global_step}\"] + [f\"{k}={np.mean(v):.2f}\" for k, v in sorted(logs.items())]))\n",
    "            logs = defaultdict(list)\n",
    "            last_log_frame = total_frames\n",
    "            last_log_time = current_log_time\n",
    "\n",
    "    envs.close()\n",
    "\n",
    "    #region: save agent checkpoint\n",
    "\n",
    "\n",
    "\n",
    "    #endregion\n",
    "\n",
    "def eval():\n",
    "    env_fn = make_env_fn(FLAGS.eval_seed, FLAGS.eval_render)\n",
    "    env = env_fn()\n",
    "\n",
    "    #region: initialize agent and load checkpoint\n",
    "\n",
    "\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode_idx in range(FLAGS.eval_num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "\n",
    "            #region: select action\n",
    "\n",
    "\n",
    "\n",
    "            #endregion\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode_idx}: \"\n",
    "                      f\"reward={info['episode']['r']}, \"\n",
    "                      f\"steps={step}, \"\n",
    "                      f\"frames={info['episode_frame_number']}\")\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    print(f\"Evaluation completed: \"\n",
    "          f\"mean_episode_reward={np.mean(episode_rewards):.2f}, \"\n",
    "          f\"std_episode_reward={np.std(episode_rewards):.2f}, \"\n",
    "          f\"min_episode_reward={np.min(episode_rewards):.2f}, \"\n",
    "          f\"max_episode_reward={np.max(episode_rewards):.2f}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb31e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if FLAGS.mode == 'train':\n",
    "        train()\n",
    "    elif FLAGS.mode == 'eval':\n",
    "        eval()\n",
    "\n",
    "print(sys.argv)\n",
    "# remove jupyter cmdline args\n",
    "sys.argv = list([sys.argv[0]])\n",
    "print(sys.argv)\n",
    "# custom flags:\n",
    "print(np.array(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(main, argv = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
