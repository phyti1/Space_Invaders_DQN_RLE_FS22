{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f29891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pipenv install\n",
    "\n",
    "import os\n",
    "if(os.path.exists(\"./persistent\")):\n",
    "    os.chdir(\"./persistent\")\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from absl import flags, app\n",
    "from tensorboardX import SummaryWriter\n",
    "import rle_assignment.env\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "from rle_assignment.utils import LinearSchedule, RingBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539573e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for key in keys_list:\n",
    "        # ignore default flags\n",
    "        if(key not in ['logtostderr', 'alsologtostderr', 'log_dir', 'v', 'verbosity', 'logger_levels', 'stderrthreshold', 'showprefixforinfo', 'run_with_pdb', 'pdb_post_mortem', 'pdb', 'run_with_profiling', 'profile_file', 'use_cprofile_for_profiling', 'only_check_args']):\n",
    "            FLAGS.__delattr__(key)\n",
    "\n",
    "# try to clear all flags to be able to rerun\n",
    "try:\n",
    "    del_all_flags(flags.FLAGS)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# common flags\n",
    "flags.DEFINE_enum('mode', 'train', ['train', 'eval'], 'Run mode.')\n",
    "flags.DEFINE_string('logdir', './runs', 'Directory where all outputs are written to.')\n",
    "flags.DEFINE_string('run_name', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'), 'Run name.')\n",
    "flags.DEFINE_bool('cuda', True, 'Whether to run the model on gpu or on cpu.')\n",
    "flags.DEFINE_integer('seed', 42, 'Random seed.')\n",
    "\n",
    "# train flags\n",
    "flags.DEFINE_integer('num_envs', 1, 'Number of parallel env processes.')\n",
    "flags.DEFINE_float('learning_rate', 2.5e-4, 'Learning rate.')\n",
    "flags.DEFINE_integer('batch_size', 32, 'Train batch size.')\n",
    "\n",
    "flags.DEFINE_integer('warmup_steps', 80_000, 'Number of warmup steps to fill the replay buffer.')\n",
    "flags.DEFINE_integer('buffer_size', 100_000, 'Replay buffer size.')\n",
    "flags.DEFINE_integer('total_steps', 100_000, 'Total number of agent steps.')\n",
    "\n",
    "flags.DEFINE_integer('checkpoint_freq', 1_000, 'Frequency at which checkpoints are stored.')\n",
    "flags.DEFINE_integer('logging_freq', 10_000, 'Frequency at which logs are written.')\n",
    "\n",
    "flags.DEFINE_float('exploration_epsilon_initial', 1.0, 'Initial exploration rate.')\n",
    "flags.DEFINE_float('exploration_epsilon_final', 0.1, 'Final exploration rate.')\n",
    "flags.DEFINE_float('exploration_fraction', 0.1, 'Fraction of total_frames it takes to decay initial to final epsilon.')\n",
    "\n",
    "\n",
    "# eval flags\n",
    "flags.DEFINE_integer('eval_num_episodes', 30, 'Number of eval episodes.')\n",
    "flags.DEFINE_bool('eval_render', False, 'Render env during eval.')\n",
    "flags.DEFINE_integer('eval_seed', 1234, 'Eval seed.')\n",
    "flags.DEFINE_string('eval_path', './2022-04-29_13-54-24/checkpoint-3500000.pt', 'relative path in logdir for evaluation')\n",
    "flags.DEFINE_float('eval_epsilon', 0.05, 'Epsilon-greedy during eval.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_fn(seed: int, render_human: bool = False, video_folder: Optional[str] = None) -> Callable[[], gym.Env]:\n",
    "    \"\"\" returns a pickleable callable to create an env instance \"\"\"\n",
    "    def env_fn():\n",
    "        env = rle_assignment.env.make_env(render_human, video_folder)\n",
    "\n",
    "        #region: maybe add other gym.wrappers\n",
    "\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.TransformObservation(env, np.squeeze)  # get rid of 3rd dimension added by ResizeObservation\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return env_fn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions: int, in_channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.qnet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = torch.unsqueeze(obs, dim=1)  # add channel dim\n",
    "        obs = obs * (1. / 255.)\n",
    "        return self.qnet(obs)\n",
    "\n",
    "def train():\n",
    "    random.seed(FLAGS.seed)\n",
    "    np.random.seed(FLAGS.seed)\n",
    "\n",
    "    logdir = os.path.join(FLAGS.logdir, FLAGS.run_name)\n",
    "    # try:\n",
    "    os.makedirs(logdir, exist_ok=False)\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    FLAGS.append_flags_into_file(os.path.join(logdir, 'flags.txt'))\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(logdir, 'logs'))\n",
    "    writer.add_text(\"config\", FLAGS.flags_into_string())\n",
    "\n",
    "    # initialize environments\n",
    "    envs = gym.vector.AsyncVectorEnv([\n",
    "        make_env_fn(seed=FLAGS.seed, video_folder=os.path.join(logdir, 'videos', 'train') if i == 0 else None)\n",
    "        for i in range(FLAGS.num_envs)])\n",
    "\n",
    "    env_name = envs.get_attr('spec')[0].name\n",
    "\n",
    "    #region: initialize agent, algorithm, etc...\n",
    "    \n",
    "    exploration_epsilon_schedule = LinearSchedule(\n",
    "        initial_value=1.,\n",
    "        final_value=FLAGS.exploration_epsilon_final,\n",
    "        schedule_steps=int(FLAGS.exploration_fraction * FLAGS.total_steps)\n",
    "    )\n",
    "\n",
    "    q_network = DQN(envs.single_action_space.n).to(device)\n",
    "    target_network = DQN(envs.single_action_space.n).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=FLAGS.learning_rate)\n",
    "\n",
    "    replay_buffer = RingBuffer(size=FLAGS.buffer_size, specs={\n",
    "        'obs': (envs.single_observation_space.shape, envs.single_observation_space.dtype),\n",
    "        'next_obs': (envs.single_observation_space.shape, envs.single_observation_space.dtype),\n",
    "        'actions': (envs.single_action_space.shape, envs.single_action_space.dtype),\n",
    "        'rewards': ((), np.float32),\n",
    "        'dones': ((), np.float32),\n",
    "    })\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    logs = defaultdict(list)\n",
    "    last_log_frame = 0\n",
    "    last_log_time = time.time()\n",
    "    total_frames = 0\n",
    "\n",
    "    obs = envs.reset()\n",
    "\n",
    "    for global_step in range(FLAGS.total_steps):\n",
    "\n",
    "        #region: select actions (one for each environment)\n",
    "\n",
    "        epsilon = exploration_epsilon_schedule.value(global_step)\n",
    "        if random.random() < epsilon:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            q_values = q_network(torch.tensor(obs).to(device))\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "        logs[\"epsilon\"].append(epsilon)\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        # execute actions in environment\n",
    "        new_obs, rewards, dones, infos = envs.step(actions)\n",
    "        for done, info in zip(dones, infos):\n",
    "            #print(actions)\n",
    "            print(f\"info: {info}\")\n",
    "            if done and \"episode\" in info.keys():\n",
    "                logs[f\"{env_name}/episode_frames\"].append(info[\"episode_frame_number\"])\n",
    "                logs[f\"{env_name}/episode_reward\"].append(info[\"episode\"][\"r\"])\n",
    "                logs[f\"{env_name}/episode_steps\"].append(info[\"episode\"][\"l\"])\n",
    "                total_frames += info[\"episode_frame_number\"]\n",
    "\n",
    "        # vector envs reset automatically, so we have to manually get the terminal observations for these steps\n",
    "        next_obs = new_obs.copy()\n",
    "        for i, done in enumerate(dones):\n",
    "            if done and infos[i].get(\"terminal_observation\") is not None:\n",
    "                next_obs[i] = infos[i][\"terminal_observation\"]\n",
    "\n",
    "        #region: update agent\n",
    "\n",
    "        replay_buffer.put({\n",
    "            'obs': obs,\n",
    "            'next_obs': next_obs,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'dones': dones,\n",
    "        })\n",
    "        # optimize model (after initial warmup phase to fill the replay buffer)\n",
    "        if global_step > FLAGS.warmup_steps and global_step % FLAGS.train_freq == 0:\n",
    "            # sample a batch from the replay buffer\n",
    "            batch = {\n",
    "                k: torch.tensor(v).to(device)\n",
    "                for k, v in replay_buffer.sample(FLAGS.batch_size).items()\n",
    "            }\n",
    "\n",
    "            # compute estimate of best q values starting from next states\n",
    "            next_q_value, _ = target_network(batch['next_obs']).max(dim=1)\n",
    "\n",
    "            # mask q values where the episode has ended at the current step\n",
    "            next_q_value_masked = next_q_value * (1 - batch['dones'])\n",
    "\n",
    "            # compute td target\n",
    "            td_target = batch['rewards'] + FLAGS.gamma * next_q_value_masked\n",
    "\n",
    "            # compute estimated q values of actions taken in current step\n",
    "            selected_q_values = q_network(batch['obs']).gather(1, torch.unsqueeze(batch['actions'], dim=1))\n",
    "\n",
    "            # compute loss (huber loss)\n",
    "            loss = F.smooth_l1_loss(selected_q_values.squeeze(), td_target.detach())\n",
    "\n",
    "            # optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(list(q_network.parameters()), FLAGS.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            logs['loss'].append(loss.cpu().detach().numpy())\n",
    "            logs['q_values'].append(selected_q_values.mean().item())\n",
    "            logs['grad_norm'].append(grad_norm.cpu().detach().numpy())\n",
    "\n",
    "        #endregion\n",
    "\n",
    "        # set obs to new obs for next step\n",
    "        obs = new_obs\n",
    "\n",
    "        # store checkpoint\n",
    "        if global_step % FLAGS.checkpoint_freq == 0:\n",
    "\n",
    "            #region: save agent checkpoint\n",
    "\n",
    "            current_log_time = time.time()\n",
    "            fps = (total_frames - last_log_frame) / (current_log_time - last_log_time)\n",
    "            writer.add_scalar(\"fps\", fps, total_frames)\n",
    "            writer.add_scalar('steps', global_step, total_frames)\n",
    "            for k, v in logs.items():\n",
    "                writer.add_scalar(f'{k}/mean', np.mean(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/std', np.std(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/min', np.min(v), total_frames)\n",
    "                writer.add_scalar(f'{k}/max', np.max(v), total_frames)\n",
    "            logs['fps'].append(fps)\n",
    "            print(\" | \".join([f\"step={global_step}\"] + [f\"{k}={np.mean(v):.2f}\" for k, v in sorted(logs.items())]))\n",
    "            logs = defaultdict(list)\n",
    "            last_log_frame = total_frames\n",
    "            last_log_time = current_log_time\n",
    "\n",
    "            #endregion\n",
    "            pass\n",
    "\n",
    "    envs.close()\n",
    "\n",
    "    #region: save agent checkpoint\n",
    "\n",
    "    torch.save(q_network.state_dict(), os.path.join(logdir, f'checkpoint-last.pt'))\n",
    "\n",
    "    #endregion\n",
    "\n",
    "def eval():\n",
    "    env_fn = make_env_fn(FLAGS.eval_seed, FLAGS.eval_render)\n",
    "    env = env_fn()\n",
    "\n",
    "    #region: initialize agent and load checkpoint\n",
    "\n",
    "    q_network = DQN(env.action_space.n).to(device)\n",
    "    q_network.load_state_dict(torch.load(os.path.join(FLAGS.logdir, FLAGS.eval_path)))\n",
    "    q_network.eval()\n",
    "\n",
    "    #endregion\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode_idx in range(FLAGS.eval_num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "\n",
    "            #region: select action\n",
    "\n",
    "            if random.random() < FLAGS.eval_epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = q_network(torch.tensor([obs]).to(device))\n",
    "                action = int(torch.argmax(q_values, dim=1).cpu().numpy())\n",
    "\n",
    "            #endregion\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode_idx}: \"\n",
    "                      f\"reward={info['episode']['r']}, \"\n",
    "                      f\"steps={step}, \"\n",
    "                      f\"frames={info['episode_frame_number']}\")\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    print(f\"Evaluation completed: \"\n",
    "          f\"mean_episode_reward={np.mean(episode_rewards):.2f}, \"\n",
    "          f\"std_episode_reward={np.std(episode_rewards):.2f}, \"\n",
    "          f\"min_episode_reward={np.min(episode_rewards):.2f}, \"\n",
    "          f\"max_episode_reward={np.max(episode_rewards):.2f}\")\n",
    "    env.close()\n",
    "\n",
    "def main(_):\n",
    "    if FLAGS.mode == 'train':\n",
    "        train()\n",
    "    elif FLAGS.mode == 'eval':\n",
    "        eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sys.argv)\n",
    "# remove jupyter cmdline args\n",
    "sys.argv = list([sys.argv[0]])\n",
    "#print(sys.argv)\n",
    "# custom flags:\n",
    "#print(np.array(FLAGS))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main, argv = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf187b75cc3fb8ab9aa91773336101b1de52e975f089fdec543117d3a8f9f64"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('rle_MC1-L-IjOXmM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
